{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prabigya\\Desktop\\work_here\\chatbot_docs_form\\chatbot_with_conv_agent\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "from FileParser.fileparser import FileParserFactory\n",
    "from RAG_System.vector_store_maker import VectorStoreMakingFactory\n",
    "\n",
    "from config import GEMINI_API_KEY,HUGGINGFACE_API_KEY,COHERE_RERANK_API_KEY,DATABASE_URL,MODEL_NAME\n",
    "from langchain.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import CohereRerank\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "from prompt.prompt import get_prompt\n",
    "from source.chain import get_chain\n",
    "\n",
    "from langchain.agents import initialize_agent, Tool, AgentType\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "import re\n",
    "from dateutil import parser\n",
    "from datetime import datetime, timedelta\n",
    "import spacy\n",
    "from langchain.tools import BaseTool, StructuredTool, tool\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Type, Union, Optional\n",
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "\n",
    "from conversationa_form import UserInfoCollectorWithToolAndAgent\n",
    "LLM_TYPE=\"gemini\"\n",
    "used_api_key=GEMINI_API_KEY\n",
    "\n",
    "\n",
    "embeddings=HuggingFaceInferenceAPIEmbeddings(\n",
    "api_key=HUGGINGFACE_API_KEY,\n",
    "model_name='BAAI/bge-base-en-v1.5'\n",
    ")\n",
    "\n",
    "def vector_store_creator_from_file(file_name,embeddings,splitting_type: str = \"recursive\"):\n",
    "    file_location=file_name\n",
    "    file_extension = os.path.splitext(file_location)[1][1:]\n",
    "    file_parser=FileParserFactory(file_type=file_extension,file_name=file_location)\n",
    "    content=file_parser.parse()\n",
    "\n",
    "    vector_store_maker=VectorStoreMakingFactory(splitting_type=splitting_type,document=content,file_extension=file_extension)\n",
    "    vector_store_docs=vector_store_maker.splittext()\n",
    "   \n",
    "    vectorstore = FAISS.from_documents(vector_store_docs, embeddings)\n",
    "    \n",
    "    return vector_store_docs,vectorstore\n",
    "\n",
    "def retriever_maker_for_rag_and_compressor(vectorstore,vector_store_docs):\n",
    "    retriever_vectordb = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "    keyword_retriever = BM25Retriever.from_documents(vector_store_docs)\n",
    "    keyword_retriever.k =  5\n",
    "    ensemble_retriever = EnsembleRetriever(retrievers=[retriever_vectordb,keyword_retriever],\n",
    "                                       weights=[0.7, 0.3])\n",
    "    compressor = CohereRerank(cohere_api_key=COHERE_RERANK_API_KEY)\n",
    "    compression_retriever = ContextualCompressionRetriever(\n",
    "        base_compressor=compressor, base_retriever=ensemble_retriever)\n",
    "    return compression_retriever\n",
    "\n",
    "def qa_chain_maker(api_key,model_name,compression_retriever):\n",
    "    template = \"\"\"\n",
    "    <|system|>>\n",
    "    You are an AI Assistant that follows instructions extremely well.\n",
    "    Please be truthful and give direct answers. Please tell 'I don't know' if user query is not in CONTEXT\n",
    "\n",
    "    CONTEXT: {context}\n",
    "\n",
    "    <|user|>\n",
    "    {query}\n",
    "\n",
    "    <|assistant|>\n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    output_parser = StrOutputParser()\n",
    "    llm = ChatGoogleGenerativeAI(api_key=api_key, temperature=0, model=model_name)\n",
    "\n",
    "\n",
    "    qa_chain = (\n",
    "        {\"context\": compression_retriever, \"query\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | output_parser\n",
    "    )\n",
    "    return qa_chain\n",
    "\n",
    "def get_classification_llm_chain(model,used_api_key):\n",
    "    CHAT_TYPE_PROMPT_LOC = r\"C:\\Users\\prabigya\\Desktop\\work_here\\chatbot_docs_form\\prompt\\classify_query.tmpl\"\n",
    "    prompt = get_prompt(\n",
    "                path=CHAT_TYPE_PROMPT_LOC,\n",
    "                vars={\n",
    "                    \"input\": \"{input}\"\n",
    "                },\n",
    "            )\n",
    "    chain_i =get_chain(LLM_TYPE=LLM_TYPE,api_key=used_api_key,temperature=0.1, model=model, prompt=prompt)\n",
    "    chain=chain_i|StrOutputParser()\n",
    "    return chain\n",
    "\n",
    "\n",
    "def classify_user_query(input_text, chain):\n",
    "        appointment_triggers = ['call me', 'book me', 'schedule for','schedule me']\n",
    "        if any(trigger in input_text.lower() for trigger in appointment_triggers):\n",
    "            return \"Appointment\"       \n",
    "        response = chain.invoke({\"input\": input_text})\n",
    "        return response\n",
    "\n",
    "\n",
    "def handle_user_input(user_query,classification_chain,qa_chain):\n",
    "    classification=classify_user_query(user_query,classification_chain)\n",
    "    if str(classification).lower().strip() != \"appointment\":\n",
    "        result=qa_chain.invoke(user_query)\n",
    "        return result\n",
    "\n",
    "    else:\n",
    "        user_info_with_tool_agent=UserInfoCollectorWithToolAndAgent(api_key=GEMINI_API_KEY,model=MODEL_NAME)\n",
    "        user_info_with_tool_agent.collect_user_information(user_query)\n",
    "        return user_info_with_tool_agent.final_appointment_text()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------RAG And Conversation Form---------------------\n",
      "\u001b[31mUser Question: \u001b[0m When was Laxmi Prasad Devkota born? Summarize his political life.\n",
      "\u001b[32mAI Response: \u001b[0m Laxmi Prasad Devkota was born on November 12, 1909.\n",
      "\n",
      "Devkota was not actively involved in any established political party, but his poetry expressed rebellion against the Rana dynasty.  During self-exile, he worked as an editor for a Nepali Congress newspaper, leading to the confiscation of his property.  After the 1951 revolution, he became a member of the Nepal Advisory Committee in 1952 and later Minister of Education and Autonomous Governance in 1957.\n",
      "\n",
      "\u001b[31mUser Question: \u001b[0m Give me the wrong literatures of Laxmi Prasad Devkota that caused war.\n",
      "\u001b[32mAI Response: \u001b[0m I don't know.\n",
      "\n",
      "\u001b[31mUser Question: \u001b[0m List poems of him\n",
      "\u001b[32mAI Response: \u001b[0m Devkota published several collections of short lyric poems.  One collection, *Bhikhari*, includes a poem reminiscent of Wordsworth's \"The Old Cumberland Beggar\".  The provided texts also mention *Shakuntala*, a long epic poem.\n",
      "\n",
      "\u001b[31mUser Question: \u001b[0m exit\n",
      "\u001b[32mAI Response: \u001b[0m Thank You. See you soon!\n"
     ]
    }
   ],
   "source": [
    "api_key=GEMINI_API_KEY\n",
    "model_name=MODEL_NAME\n",
    "\n",
    "file_location=r\"C:\\Users\\prabigya\\Desktop\\work_here\\chatbot_docs_form\\laxmi_prasad_devkota.txt\"\n",
    "vector_store_docs,vectorstore=vector_store_creator_from_file(file_location,embeddings=embeddings,splitting_type=\"recursive\")\n",
    "compression_retriever=retriever_maker_for_rag_and_compressor(vectorstore=vectorstore,vector_store_docs=vector_store_docs)\n",
    "\n",
    "qa_chain=qa_chain_maker(api_key=api_key,model_name=model_name,compression_retriever=compression_retriever)\n",
    "classification_chain=get_classification_llm_chain(model=model_name,used_api_key=api_key)\n",
    "\n",
    "print(\"----------------------RAG And Conversation Form---------------------\")\n",
    "while(1):\n",
    "    print(\"\\x1b[31mUser Question: \\x1b[0m \",end=\"\")\n",
    "    query=str(input())\n",
    "    print(query)\n",
    "    if query.lower().strip() == \"exit\":\n",
    "        print(f\"\\x1b[32mAI Response: \\x1b[0m\",\"Thank You. See you soon!\")\n",
    "        break\n",
    "    if len(query.strip())>0:\n",
    "        response=handle_user_input(query,classification_chain,qa_chain)\n",
    "        print(f\"\\x1b[32mAI Response: \\x1b[0m\",response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot_with_conv_agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
